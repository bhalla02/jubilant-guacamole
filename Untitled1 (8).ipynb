{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7382f01d-72aa-4091-82ab-b45fe60e06a9",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "In machine learning, kernel functions allow algorithms to operate in a high-dimensional feature space without explicitly computing the coordinates of the data in that space. Polynomial functions are a specific type of kernel function that can be used to map the original features into a higher-dimensional space.\n",
    "\n",
    "The polynomial kernel function is defined as:\n",
    "\n",
    "ùêæ\n",
    "(\n",
    "ùë•\n",
    "ùëñ\n",
    ",\n",
    "ùë•\n",
    "ùëó\n",
    ")\n",
    "=\n",
    "(\n",
    "ùë•\n",
    "ùëñ\n",
    "ùëá\n",
    "ùë•\n",
    "ùëó\n",
    "+\n",
    "ùëê\n",
    ")\n",
    "ùëë\n",
    "K(x \n",
    "i\n",
    "‚Äã\n",
    " ,x \n",
    "j\n",
    "‚Äã\n",
    " )=(x \n",
    "i\n",
    "T\n",
    "‚Äã\n",
    " x \n",
    "j\n",
    "‚Äã\n",
    " +c) \n",
    "d\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "ùë•\n",
    "ùëñ\n",
    "x \n",
    "i\n",
    "‚Äã\n",
    "  and \n",
    "ùë•\n",
    "ùëó\n",
    "x \n",
    "j\n",
    "‚Äã\n",
    "  are input feature vectors.\n",
    "ùëê\n",
    "c is a constant term that controls the influence of higher-order versus lower-order terms.\n",
    "ùëë\n",
    "d is the degree of the polynomial.\n",
    "This kernel function allows the SVM to create non-linear decision boundaries by transforming the data into a higher-dimensional space where a linear separator might be more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8e8e7e-6474-4e2c-93c0-65939c09990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# We will only use the first two classes for binary classification\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM with a polynomial kernel\n",
    "clf = SVC(kernel='poly', degree=3, C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15fb35d-a2de-45a8-a4e4-2b763403905d",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "In Support Vector Regression (SVR), the epsilon parameter (\n",
    "ùúñ\n",
    "œµ) defines a margin of tolerance where no penalty is given to errors. When \n",
    "ùúñ\n",
    "œµ increases, the margin of tolerance becomes wider, allowing more data points to fall within this margin without contributing to the loss function. As a result, the number of support vectors typically decreases because fewer data points are outside the epsilon margin and thus influence the model.\n",
    "\n",
    "\n",
    "\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "Kernel function: The choice of kernel (e.g., linear, polynomial, RBF) determines the way the input space is transformed.\n",
    "\n",
    "Use a linear kernel for linearly separable data.\n",
    "Use a polynomial kernel for data with polynomial relationships.\n",
    "Use an RBF kernel for complex data patterns.\n",
    "C parameter: The regularization parameter \n",
    "ùê∂\n",
    "C controls the trade-off between achieving a low error on the training data and minimizing the model complexity.\n",
    "\n",
    "A high C value means fewer margins and less tolerance for misclassified points, which can lead to overfitting.\n",
    "A low C value allows more flexibility, which can lead to underfitting.\n",
    "Epsilon parameter: The epsilon parameter (\n",
    "ùúñ\n",
    "œµ) in SVR defines the margin within which no penalty is given to errors.\n",
    "\n",
    "A large epsilon creates a wider margin, which can reduce the number of support vectors and might underfit the data.\n",
    "A small epsilon results in a narrower margin, potentially increasing the number of support vectors and risk overfitting.\n",
    "Gamma parameter: In kernels like RBF and polynomial, the gamma parameter defines how far the influence of a single training example reaches.\n",
    "\n",
    "A low gamma means a larger influence, which can lead to underfitting.\n",
    "A high gamma means a smaller influence, which can lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a61796b-d6f3-49d5-a7c8-7bbf44e8a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1f5271-6244-44f9-94e3-7e8dc4008f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09860625-030a-4c0d-a144-2e17e6c64657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n",
      "Precision: 0.9793650793650793\n",
      "Recall: 0.9777777777777777\n",
      "F1-Score: 0.9777448559670783\n",
      "Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_svc_model.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svc = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'best_svc_model.joblib')\n",
    "\n",
    "# To load the model in the future, use:\n",
    "# best_svc = joblib.load('best_svc_model.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d41e7-e993-4ca4-828e-d72ecc103b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
